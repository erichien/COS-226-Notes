# COS 226 

More reference:
https://www.cs.princeton.edu/courses/archive/spring13/cos423/lectures.php





[TOC]





# L1: Union Find

## Quick Find (Original Union Find)

### Find (Quick Find): 
return root( node ). a.k.a. Find the root of a given node, O(1) time cause all connected nodes are assigned to the same root  in Union operation

### Union: O(N)
assign all connected nodes one identical root by looping through all nodes every time Union operation is called, and updating two to-be-connected groups of connected components with the new root

## Improvements

### Quick Union: O(N)
only point a to-be-joined node (or the root of the to-be-joined tree) to the root (node) of the other, instead of changing the root for all components in the trees (downside: Find operation becomes O(N), since we need to traverse from a node back to its root)

### Weighted Quick Union (Union by Size): O(log N)
point root of the smaller tree to the root of the larger tree. This way the resulting union tree will have more nodes at the top level than in the bottom -> more nodes will be closer to root, takes less time to find. More scalable.  (Find operation becomes O(log N), since depth of any node is at most log N (N = number of all nodes) ) we need an extra size[] array to keep track of the number of components in each tree (e.g. i = root(p), size[i] is the number of nodes in this tree.) Before merging, we check the number of nodes in each tree by comparing size[ root 1 ] & size[ root 2 ], and updating with size[ root of larger tree ] = size[ root 1 ] + size[ root 2 ] after merging. See screenshot below.

### Union by Rank (depth of tree): O(log N)
	UNION-BY-RANK (x, y) 
	r ← FIND (x). 
	s ← FIND (y). 
	IF (r = s) :
		RETURN. 
	ELSE IF rank(r) > rank(s) :
		parent(s) ← r. 
	ELSE IF rank(r) < rank(s) :
		parent(r) ← s. 
	ELSE :
		parent(r) ← s. 
		rank(s) ← rank(s) + 1. 

### Path compression:
when traversing nodes in Find/root() operation, point each node to its grandparent node, instead of its already connected parent node, to make the tree flatter

### Applications:
Detecting percolation

More:
https://www.cs.princeton.edu/courses/archive/spring13/cos423/lectures/UnionFind.pdf



![Screen Shot 2018-01-28 at 5.58.27 PM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/0E2A3A70-517B-4FF7-B62B-7B0856AE2E54/Screen Shot 2018-01-28 at 5.58.27 PM.png)

![Screen Shot 2018-01-28 at 5.57.48 PM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/BCC90614-B747-464B-9962-9F06CA4E2EC1/Screen Shot 2018-01-28 at 5.57.48 PM.png)

￼![Screen Shot 2018-01-28 at 8.45.25 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/08E0FA67-77CF-411D-8A50-C501931BDAAF/Screen Shot 2018-01-28 at 8.45.25 AM.png)

![Screen Shot 2018-01-28 at 8.27.34 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/FA12AEE7-69FB-4FE6-86FD-7D670607858A/Screen Shot 2018-01-28 at 8.27.34 AM.png)

![Screen Shot 2018-01-28 at 8.27.41 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/CD0F2BC1-5975-4C29-B199-B2B6944DB13A/Screen Shot 2018-01-28 at 8.27.41 AM.png)

![Screen Shot 2018-01-28 at 8.36.09 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/5D3F27F4-BD87-4B47-8BF7-4A62591F2E5F/Screen Shot 2018-01-28 at 8.36.09 AM.png)

![Screen Shot 2018-01-28 at 8.39.07 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/E94FDC0F-D4CD-4B67-BB56-AFC61E65CAB6/Screen Shot 2018-01-28 at 8.39.07 AM.png)

![Screen Shot 2018-01-28 at 8.38.36 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/23241538-960C-48C8-9EA3-2CBD52EAB256/Screen Shot 2018-01-28 at 8.38.36 AM.png)

![Screen Shot 2018-01-30 at 8.24.49 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/0BEB5698-D14C-4AC6-80E8-946496F2AD48/Screen Shot 2018-01-30 at 8.24.49 AM.png)

![Screen Shot 2018-01-28 at 11.38.43 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/715F8A90-2C5A-4A61-894A-641EA89AF593/Screen Shot 2018-01-28 at 11.38.43 AM.png)

![Screen Shot 2018-01-28 at 11.46.14 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/7CD9B46D-F62D-49D0-A5BF-B15B8193BDDB/Screen Shot 2018-01-28 at 11.46.14 AM.png)

![Screen Shot 2018-01-28 at 11.46.49 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/15B39ACD-58C0-4A24-9CB3-1A036E606139/Screen Shot 2018-01-28 at 11.46.49 AM.png)

![Screen Shot 2018-01-28 at 11.50.02 AM](/Users/eric/Library/Group Containers/group.com.apple.notes/Media/D4EA4546-1392-48CF-B3CC-22F738E90583/Screen Shot 2018-01-28 at 11.50.02 AM.png)





# L2 Analysis of Algorithms

![Screen Shot 2018-02-19 at 12.08.53 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 12.08.53 AM.png)

![Screen Shot 2018-02-19 at 1.31.50 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.31.50 PM.png)

![Screen Shot 2018-02-19 at 1.29.15 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.32.09 PM.png)

## Estimating Time Complexity with Doubling Hypothesis

a*(2*n)^b / a*(n)^b = T1 / T0

=> 2^b = T1/T0

easier to esitimate O(n) or above (e.g. O(n^2) …), 
to estimate O(log n), need to experiment with different input values / close to 0 ? 

![Screen Shot 2018-02-19 at 1.11.51 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.11.51 PM.png)

## Time Complexity Analysis

### The Meaning of Best, Worst, and Average case

![Screen Shot 2018-02-19 at 2.15.06 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 2.15.06 PM.png)

### The Meaning of ~, O, Ø, Ω Notation

"~": keeping the leading (most powerful, e.g. N^2) term, neglecting lower terms (e.g. N, log N)

"O": worst case of an algorithm

"Ø": growth rate that can be strictly estimated (O = Ω), when it approximates/converges to a certain value

"Ω": the theoretic lower bound of how much time a task at least needs. e.g. a 3 sum problem. we at least have to have seen all array elements => O(N)

![Screen Shot 2018-02-19 at 6.20.07 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 6.20.07 PM.png)

![Screen Shot 2018-02-19 at 5.54.07 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 5.54.07 PM.png)

![Screen Shot 2018-02-19 at 6.15.36 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 6.15.36 PM.png)

## Memory

![Screen Shot 2018-02-19 at 6.33.07 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 6.33.07 PM.png)

![Screen Shot 2018-02-19 at 2.13.10 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 2.13.10 PM.png)

Objects, although customizable, cost more memory

![Screen Shot 2018-02-19 at 7.13.34 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 7.13.34 PM.png)

## Binary Search

![Screen Shot 2018-02-19 at 1.57.18 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.57.18 PM.png)

## 2-Sum, 3-Sum, and Improvements

![Screen Shot 2018-02-19 at 1.26.21 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.26.21 PM.png)

![Screen Shot 2018-02-19 at 1.26.50 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.26.50 PM.png)

![Screen Shot 2018-02-19 at 1.58.39 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 1.58.39 PM.png)





# L3 Bags, Queues, and Stacks 

## Client, Implementation, Interface (API)

![Screen Shot 2018-02-20 at 12.09.15 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 12.09.15 AM.png)

![Screen Shot 2018-02-19 at 8.27.18 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 8.27.18 PM.png)

![Screen Shot 2018-02-20 at 11.34.06 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.34.06 AM.png)

## Stack: Linked List Implementation

![Screen Shot 2018-02-19 at 8.38.07 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 8.38.07 PM.png)

![Screen Shot 2018-02-19 at 8.41.27 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 8.41.27 PM.png)

![Screen Shot 2018-02-19 at 8.41.58 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 8.41.58 PM.png)

![Screen Shot 2018-02-19 at 8.42.56 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 8.42.56 PM.png)

![Screen Shot 2018-02-19 at 9.52.40 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 9.52.40 PM.png)

## Stack: Array Implementation

![Screen Shot 2018-02-19 at 9.58.58 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 9.58.58 PM.png)

![Screen Shot 2018-02-19 at 10.08.37 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 10.08.37 PM.png)

![Screen Shot 2018-02-19 at 10.12.35 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 10.12.35 PM.png)

### Resizing the Array: Growing & Shrinking

![Screen Shot 2018-02-19 at 11.24.16 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-19 at 11.24.16 PM.png)

Resizing array: Usually good strategy could be to multiply size by 2 (or *1.5*, if you more concerned about memory over performance). A growth rate of 2 is less memory efficient, and usually have more vacency.

![Screen Shot 2018-02-20 at 12.15.53 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 12.15.53 AM.png)

![Screen Shot 2018-02-20 at 12.36.16 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 12.36.16 AM.png)

![Screen Shot 2018-02-20 at 12.49.07 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 12.49.07 AM.png)

![Screen Shot 2018-02-20 at 1.11.48 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 1.11.48 AM.png)

## Stack Implementations: Resizing Array v.s. Linked List

Linked list: when we need consistent O(1) performance

Resizing array: for less total amount of time (b.c. less operation) 

![Screen Shot 2018-02-20 at 1.18.55 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 1.18.55 AM.png)

## Implementing a Generic Stack

![Screen Shot 2018-02-20 at 10.18.33 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 10.18.33 AM.png)

## Queue: Linked List Implementation

![Screen Shot 2018-02-20 at 9.49.30 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 9.49.30 AM.png)

## Queue: Resizing Array Implementation

![Screen Shot 2018-02-20 at 9.51.07 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 9.51.07 AM.png)

## Bag

![Screen Shot 2018-02-20 at 10.33.05 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 10.33.05 AM.png)

## Iteration, Iterables, Iterators

![Screen Shot 2018-02-20 at 10.29.39 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 10.29.39 AM.png)

![Screen Shot 2018-02-20 at 10.30.00 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 10.30.00 AM.png)

### A Stack Iterator

![Screen Shot 2018-02-20 at 10.32.17 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 10.32.17 AM.png)

## Applications

![Screen Shot 2018-02-20 at 11.32.25 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.32.25 AM.png)

![Screen Shot 2018-02-20 at 11.32.43 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.32.43 AM.png)

![Screen Shot 2018-02-20 at 11.36.17 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.36.17 AM.png)

![Screen Shot 2018-02-20 at 11.36.43 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.36.43 AM.png)

![Screen Shot 2018-02-20 at 11.37.30 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.37.30 AM.png)

![Screen Shot 2018-02-20 at 11.37.58 AM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-20 at 11.37.58 AM.png)









# L6 Quicksort

## Quicksort

### Time complexity: O( N lg N )

Since the expected number of compares on average is ~1.39 N lg N 

Quicksort has 39% more compares than mergesort, but it's faster than mergesort in practice because of less data movement (operations), quicksort simply moves pointer forward.

Quicksort is not stable, but shuffling the input array guarantees better performance.

### Space complexity

Quicksort is an in-place sorting algorithm. 

Partitioning: Constant extra space, a.k.a no extra space

Depth of recursion: logarithmic extra space (can be approximated with random shuffling input)

![Screen Shot 2018-02-18 at 12.38.04 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 12.38.04 PM.png)

![Screen Shot 2018-02-18 at 12.37.48 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 12.37.48 PM.png)

## Quick select (select Kth largest)

Min (k = 0), max (k = N-1)

If k was small (e.g. 1st, 2nd, 3rd smallest) or large (e.g. N-1, N-2 smallest, = 1st, 2nd largest)

=> Time complexity ~Ø(n), since we only have to loop over once, keep track of top few num

Otherwise, we can always get O( N log N ) time complexity with sorting.

With Quick Select, we can get O(N) linear time on average

### Time complexity: O(N) on average

Time complexity: N + N/2 + N/4 + … + 1 ~ 2N => O(N) on average

~ 1/2 N^2 compares in the worst case

Use quick select if a full-sort isn't needed!

despite being linear time complexity, the operations of quick select still make the constants too high => not used in practice. A practical linear-time worst case algorithm still needs to be discoverd.

## Improvements

### Insertion sort on small subarrays

![Screen Shot 2018-02-18 at 1.38.19 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 1.38.19 PM.png)

### Choose median as pivot item

![Screen Shot 2018-02-18 at 1.38.36 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 1.38.36 PM.png)

### Best case analysis

paritition step divide every subarray exactly in half => ~ N lg N compares

![Screen Shot 2018-02-18 at 12.45.02 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 12.45.02 PM.png)

### Worst case analysis

input array already sorted/reverse sorted. Since we always pick the left most item as the Pivot item, during partition after we scan through the arraying, since everything is sorted, no elements are swaped, only the pivot item is eliminated everytime. Therefore we are scanning the N +N-1 + N-2 + … 1 number of items in total 

=> ~ 1/2 N^2 compares, and thats why we need to shuffle input array

![Screen Shot 2018-02-18 at 12.46.46 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 12.46.46 PM.png)

### Duplicate Keys: 3-way Quicksort w/ Dijkstra 3-way partitioning

When duplicates are present, it is (counter-intuitively) better to stop scans on keys equal to the partitioning item’s key. Time goes quadratic O(N^2) if the algorithm doesn't stop partitioning when on equal keys. 

Quicksort with 3-way partitioning reduces running time from linearithmic O(N lg N) to linear O(N) in broad class of applications.

![Screen Shot 2018-02-18 at 7.52.03 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.52.03 PM.png)

![Screen Shot 2018-02-18 at 6.42.57 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 6.42.57 PM.png)

![Screen Shot 2018-02-18 at 6.43.06 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 6.43.06 PM.png)

![Screen Shot 2018-02-18 at 6.43.23 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 6.43.23 PM.png)

### In Practice: Bentley-Mcilroy 3-way partitioning

Tukey's ninther does better partitioning than random shuffle, and is less costly.

But since it doesn't do the random shuffle, it can overflow function call stack and crash in rare circumstances 

=> not completely solid, shuffling is needed

![Screen Shot 2018-02-18 at 7.35.16 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.35.16 PM.png)

![Screen Shot 2018-02-18 at 7.40.29 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.40.29 PM.png)

### In Practice: Dual-pivot quicksort

Now widely used. Java 8, Python unstable sort, Android, …

![Screen Shot 2018-02-18 at 7.56.39 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.56.39 PM.png)

![Screen Shot 2018-02-18 at 7.57.15 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.57.15 PM.png)

![Screen Shot 2018-02-18 at 7.57.41 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.57.41 PM.png)

![Screen Shot 2018-02-18 at 7.58.12 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.58.12 PM.png)

![Screen Shot 2018-02-18 at 7.58.38 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.58.38 PM.png)





# Sorting Summary, System Sorts

![Screen Shot 2018-02-18 at 7.14.04 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.14.04 PM.png)

![Screen Shot 2018-02-18 at 7.44.45 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.44.45 PM.png)

![Screen Shot 2018-02-18 at 7.45.15 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 7.45.15 PM.png)

## Q: Why use different algorithms for primitive and reference types? 

### Dual-pivot quicksort is used for primitive types, Timsort (mergesort + insertion sort) for reference types (objects)

A: Implementors of Java believe that, for programmers using objects, space may not be critically important, therefore the extra space used by mergesort is probably not a problem. For programmers using the primitive types, performance (quicksort) might be the most important thing.



![Screen Shot 2018-02-18 at 6.50.54 PM](/Users/eric/Desktop/COS 226/Screen Shot 2018-02-18 at 6.50.54 PM.png)







# L10 Hash Tables 


Hash function

Hash table

Linear probing: 

when put() we might have to wrap around the array (loop back) to the 0th index if empty cells are not found, therefore when incrementing index, we use i = (i + 1) % m, same for get()
When get() key aren’t found until encountering an empty cell, it means the key doesn’t exist in the array / table, Therefore, Deletion in hash table while using linear probing requires extra care

Separate chaining vs. linear probing 

Separate chaining. 
・Performance degrades gracefully. 
・Clustering less sensitive to poorly-designed hash function. 

Linear probing. 
・Less wasted space. 
・Better cache performance.